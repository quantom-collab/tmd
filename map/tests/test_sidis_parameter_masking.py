#!/usr/bin/env python3
"""
Test suite for fNP parameter masking functionality

This module contains unit tests and integration tests for the parameter masking
system in the fNP module. The tests verify that the free_mask configuration
correctly controls which parameters are trainable during optimization.

Usage:
    python test_sidis_parameter_masking.py
    python -m pytest test_sidis_parameter_masking.py -v

Author: Generated for TMD analysis framework
"""

import os
import sys
import torch
import yaml
import unittest
from typing import Dict, Any, Optional

# Add the parent directory to the path to import modules
sys.path.append("/Users/cbissolotti/anl/projects/tmd/map")

from modules.fNP import fNP

# Check if SIDIS computation is available (optional)
try:
    from sidis_computation_pytorch import SIDISComputationPyTorch

    SIDIS_AVAILABLE = True
except ImportError:
    SIDIS_AVAILABLE = False


class TestParameterMasking(unittest.TestCase):
    """Test cases for parameter masking functionality."""

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.config_file = "/Users/cbissolotti/anl/projects/tmd/map/inputs/config.yaml"
        self.fnp_config_file = (
            "/Users/cbissolotti/anl/projects/tmd/map/inputs/fNPconfig.yaml"
        )
        self.fnp_config_test_file = (
            "/Users/cbissolotti/anl/projects/tmd/map/inputs/fNPconfig_test.yaml"
        )

        # Check if required files exist
        if not os.path.exists(self.config_file):
            self.skipTest(f"Config file not found: {self.config_file}")
        if not os.path.exists(self.fnp_config_file):
            self.skipTest(f"fNP config file not found: {self.fnp_config_file}")

    def load_yaml_config(self, config_file: str) -> Dict[str, Any]:
        """Load YAML configuration file."""
        with open(config_file, "r") as f:
            config = yaml.safe_load(f)
        return config if isinstance(config, dict) else {}

    def test_fnp_standalone_parameter_masking(self):
        """Test parameter masking directly with fNP module."""
        print("\n🧪 Testing fNP standalone parameter masking...")

        # Test with original config (all trainable except some)
        config = self.load_yaml_config(self.fnp_config_file)
        model = fNP(config)

        param_info = model.get_parameter_info()

        # Verify parameter counts make sense
        self.assertGreater(param_info["total_parameters"], 0)
        self.assertGreater(param_info["truly_trainable_parameters"], 0)
        self.assertLessEqual(
            param_info["truly_trainable_parameters"], param_info["total_parameters"]
        )

        print(
            f"✅ Original config: {param_info['truly_trainable_parameters']}/{param_info['total_parameters']} trainable"
        )

    def test_fnp_mixed_masking_configuration(self):
        """Test fNP with mixed fixed/trainable configuration."""
        if not os.path.exists(self.fnp_config_test_file):
            self.skipTest(f"Test config file not found: {self.fnp_config_test_file}")

        print("\n🧪 Testing fNP mixed masking configuration...")

        config = self.load_yaml_config(self.fnp_config_test_file)
        model = fNP(config)

        param_info = model.get_parameter_info()

        # Should have fewer trainable parameters than total
        self.assertLess(
            param_info["truly_trainable_parameters"], param_info["total_parameters"]
        )

        # Check specific flavor configurations
        flavors = param_info["flavors"]

        # u-quark should have some fixed parameters (according to test config)
        if "u" in flavors:
            u_info = flavors["u"]
            self.assertGreater(
                u_info["fixed"], 0, "u-quark should have some fixed parameters"
            )
            self.assertGreater(
                u_info["trainable"], 0, "u-quark should have some trainable parameters"
            )

        print(
            f"✅ Mixed config: {param_info['truly_trainable_parameters']}/{param_info['total_parameters']} trainable"
        )

    def test_sidis_computation_parameter_masking(self):
        """Test parameter masking within SIDIS computation context."""
        if not SIDIS_AVAILABLE:
            self.skipTest(
                "SIDIS computation dependencies not available (lhapdf, apfelpy)"
            )

        print("\n🧪 Testing parameter masking in SIDIS computation...")
        print("  ⚠️  Skipped: Missing dependencies")

        # This test would verify parameter masking in the full SIDIS context
        # but requires lhapdf and apfelpy dependencies
        pass

    def _test_parameter_masking_on_model(
        self, model: fNP, dtype: torch.dtype, device: torch.device
    ):
        """
        Core parameter masking test logic.

        This method performs a small gradient step and checks that only
        free parameters are updated while fixed parameters remain unchanged.
        """
        print("  → Testing gradient masking behavior...")

        # Store initial parameter values
        initial_params = {}
        for name, param in model.named_parameters():
            initial_params[name] = param.data.clone()

        # Create dummy input tensors
        x = torch.tensor([0.1, 0.2], dtype=dtype, device=device)
        b = torch.tensor([0.5, 1.0], dtype=dtype, device=device)

        # Forward pass
        outputs = model(x, b)
        self.assertIsInstance(
            outputs, dict, "Model should return dictionary of outputs"
        )
        self.assertGreater(
            len(outputs), 0, "Model should return at least one flavor output"
        )

        # Compute a dummy loss (sum of all outputs)
        loss = torch.tensor(0.0, dtype=dtype, device=device, requires_grad=True)
        for output in outputs.values():
            loss = loss + output.sum()

        # Backward pass
        loss.backward()

        # Check that gradients were computed
        has_gradients = False
        for param in model.parameters():
            if param.grad is not None and param.grad.norm() > 0:
                has_gradients = True
                break
        self.assertTrue(has_gradients, "Model should have computed gradients")

        # Small gradient step (manual update to test masking)
        with torch.no_grad():
            for param in model.parameters():
                if param.grad is not None:
                    param.data -= 0.01 * param.grad

        # Check which parameters changed
        changes_detected = False
        for name, param in model.named_parameters():
            initial = initial_params[name]
            changed = not torch.allclose(initial, param.data, atol=1e-8)
            if changed:
                changes_detected = True
                max_change = (param.data - initial).abs().max().item()
                print(f"    {name}: CHANGED (max_change: {max_change:.2e})")
            else:
                print(f"    {name}: UNCHANGED")

        self.assertTrue(
            changes_detected, "At least some parameters should have changed"
        )

        # Reset parameters to initial values
        with torch.no_grad():
            for name, param in model.named_parameters():
                param.data.copy_(initial_params[name])

        # Zero gradients
        model.zero_grad()

        print("  ✅ Parameter masking test completed successfully")

    def test_parameter_info_consistency(self):
        """Test that parameter info methods return consistent results."""
        print("\n🧪 Testing parameter info consistency...")

        config = self.load_yaml_config(self.fnp_config_file)
        model = fNP(config)

        # Get parameter info
        param_info = model.get_parameter_info()

        # Check consistency between different counting methods
        pytorch_total = sum(p.numel() for p in model.parameters())
        pytorch_trainable = sum(
            p.numel() for p in model.parameters() if p.requires_grad
        )

        self.assertEqual(param_info["pytorch_trainable_parameters"], pytorch_trainable)

        # Total should be sum of evolution + all flavors
        expected_total = param_info["evolution"]["total"]
        for flavor_info in param_info["flavors"].values():
            expected_total += flavor_info["total"]

        self.assertEqual(param_info["total_parameters"], expected_total)

        print(
            f"  ✅ Parameter counts consistent: {param_info['total_parameters']} total, {param_info['truly_trainable_parameters']} trainable"
        )

    def test_trainable_parameters_dict_methods(self):
        """Test get/set trainable parameters dictionary methods."""
        print("\n🧪 Testing trainable parameters dict methods...")

        config = self.load_yaml_config(self.fnp_config_file)
        model = fNP(config)

        # Get trainable parameters
        trainable_params = model.get_trainable_parameters_dict()

        # Check that we get a dictionary with expected keys
        self.assertIsInstance(trainable_params, dict)
        self.assertIn("evolution.g2", trainable_params)

        # Check that flavor parameters are present
        flavor_keys_found = [
            key for key in trainable_params.keys() if key.startswith("flavors.")
        ]
        self.assertGreater(len(flavor_keys_found), 0, "Should have flavor parameters")

        # Store original parameters for comparison
        original_params = {}
        for name, param in model.named_parameters():
            original_params[name] = param.data.clone()

        # Modify the trainable parameters slightly
        modified_trainable = {}
        for key, tensor in trainable_params.items():
            modified_trainable[key] = tensor + 0.01  # Small modification

        # Set the modified parameters
        model.set_trainable_parameters_dict(modified_trainable)

        # Check that parameters actually changed
        changes_detected = False
        for name, param in model.named_parameters():
            if not torch.allclose(original_params[name], param.data, atol=1e-6):
                changes_detected = True
                break

        self.assertTrue(
            changes_detected, "Setting trainable parameters should change model state"
        )

        # Reset to original
        model.set_trainable_parameters_dict(trainable_params)

        print("  ✅ Trainable parameters dict methods working correctly")


def run_parameter_masking_integration_test():
    """Run a comprehensive integration test similar to the original test method."""
    print("\n" + "=" * 70)
    print("🔬 COMPREHENSIVE PARAMETER MASKING INTEGRATION TEST")
    print("=" * 70)

    configs = [
        (
            "/Users/cbissolotti/anl/projects/tmd/map/inputs/fNPconfig.yaml",
            "Original (all trainable)",
        ),
        (
            "/Users/cbissolotti/anl/projects/tmd/map/inputs/fNPconfig_test.yaml",
            "Test (mixed fixed/trainable)",
        ),
    ]

    for config_file, description in configs:
        if not os.path.exists(config_file):
            print(f"❌ Config file not found: {config_file}")
            continue

        print(f"\n📋 Testing: {description}")
        print(f"   Config: {config_file}")
        print("-" * 50)

        try:
            # Load configuration
            with open(config_file, "r") as f:
                config = yaml.safe_load(f)

            if not isinstance(config, dict):
                print(f"❌ Invalid config format in {config_file}")
                continue

            # Create fNP model
            model = fNP(config)

            # Print parameter summary
            model.print_parameter_summary()

            # Test gradient masking
            print("🧪 Testing gradient masking...")

            # Store initial parameters
            initial_params = {}
            for name, param in model.named_parameters():
                initial_params[name] = param.data.clone()

            # Create dummy inputs
            x = torch.tensor([0.1, 0.2], dtype=torch.float32)
            b = torch.tensor([0.5, 1.0], dtype=torch.float32)

            # Forward pass
            outputs = model(x, b)

            # Compute loss
            loss = torch.tensor(0.0, requires_grad=True)
            for output in outputs.values():
                loss = loss + output.sum()

            # Backward pass
            loss.backward()

            # Check gradients
            print("\n   Gradient analysis:")
            for name, param in model.named_parameters():
                if param.grad is not None:
                    grad_norm = param.grad.norm().item()
                    print(f"     {name}: grad_norm = {grad_norm:.2e}")
                else:
                    print(f"     {name}: no gradient")

            # Manual parameter update to test masking
            with torch.no_grad():
                for param in model.parameters():
                    if param.grad is not None:
                        param.data -= 0.01 * param.grad

            # Check which parameters actually changed
            print("\n   Parameter change analysis:")
            for name, param in model.named_parameters():
                initial = initial_params[name]
                max_change = (param.data - initial).abs().max().item()
                changed = max_change > 1e-8
                print(
                    f"     {name}: {'CHANGED' if changed else 'UNCHANGED'} (max_change: {max_change:.2e})"
                )

            model.zero_grad()

        except Exception as e:
            print(f"❌ Error testing {description}: {e}")
            import traceback

            traceback.print_exc()

    print("\n" + "=" * 70)
    print("✅ Comprehensive parameter masking test completed!")
    print("\nKey Findings:")
    print("• free_mask controls which parameters are trainable")
    print("• Fixed parameters (free_mask=false) should not change during optimization")
    print("• Gradient hooks ensure fixed parameters receive zero gradients")
    print("• This mechanism is crucial for constrained optimization in TMD fitting")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    # Run integration test first
    run_parameter_masking_integration_test()

    # Then run unit tests
    print("\n" + "=" * 50)
    print("🧪 RUNNING UNIT TESTS")
    print("=" * 50)
    unittest.main(verbosity=2, exit=False)

    print("\n✅ All parameter masking tests completed!")
